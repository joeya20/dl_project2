{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a02285e6",
      "metadata": {
        "id": "a02285e6"
      },
      "source": [
        "# Starter Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdcc5329",
      "metadata": {
        "id": "bdcc5329"
      },
      "source": [
        "Install and import required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7",
      "metadata": {
        "id": "348ceed6-b684-46c3-8a32-9bb640c9a9d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in ./env/lib/python3.11/site-packages (4.51.1)\n",
            "Requirement already satisfied: datasets in ./env/lib/python3.11/site-packages (3.5.0)\n",
            "Requirement already satisfied: evaluate in ./env/lib/python3.11/site-packages (0.4.3)\n",
            "Requirement already satisfied: accelerate in ./env/lib/python3.11/site-packages (1.6.0)\n",
            "Requirement already satisfied: peft in ./env/lib/python3.11/site-packages (0.15.1)\n",
            "Requirement already satisfied: trl in ./env/lib/python3.11/site-packages (0.16.1)\n",
            "Requirement already satisfied: bitsandbytes in ./env/lib/python3.11/site-packages (0.45.5)\n",
            "Requirement already satisfied: filelock in ./env/lib/python3.11/site-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./env/lib/python3.11/site-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in ./env/lib/python3.11/site-packages (from transformers) (2.2.4)\n",
            "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in ./env/lib/python3.11/site-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in ./env/lib/python3.11/site-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in ./env/lib/python3.11/site-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./env/lib/python3.11/site-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in ./env/lib/python3.11/site-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in ./env/lib/python3.11/site-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in ./env/lib/python3.11/site-packages (from datasets) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in ./env/lib/python3.11/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in ./env/lib/python3.11/site-packages (from datasets) (2.2.3)\n",
            "Requirement already satisfied: xxhash in ./env/lib/python3.11/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in ./env/lib/python3.11/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.12.0,>=2023.1.0 in ./env/lib/python3.11/site-packages (from datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in ./env/lib/python3.11/site-packages (from datasets) (3.11.16)\n",
            "Requirement already satisfied: psutil in ./env/lib/python3.11/site-packages (from accelerate) (7.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in ./env/lib/python3.11/site-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: rich in ./env/lib/python3.11/site-packages (from trl) (14.0.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./env/lib/python3.11/site-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./env/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in ./env/lib/python3.11/site-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in ./env/lib/python3.11/site-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in ./env/lib/python3.11/site-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in ./env/lib/python3.11/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./env/lib/python3.11/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in ./env/lib/python3.11/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in ./env/lib/python3.11/site-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./env/lib/python3.11/site-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in ./env/lib/python3.11/site-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in ./env/lib/python3.11/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: nvidia-ml-py3 in ./env/lib/python3.11/site-packages (7.352.0)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Requirement already satisfied: scikit-learn in ./env/lib/python3.11/site-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in ./env/lib/python3.11/site-packages (3.10.1)\n",
            "Collecting seaborn\n",
            "  Obtaining dependency information for seaborn from https://files.pythonhosted.org/packages/83/11/00d3c3dfc25ad54e731d91449895a79e4bf2384dc3ac01809010ba88f6d5/seaborn-0.13.2-py3-none-any.whl.metadata\n",
            "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in ./env/lib/python3.11/site-packages (from scikit-learn) (2.2.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in ./env/lib/python3.11/site-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in ./env/lib/python3.11/site-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in ./env/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in ./env/lib/python3.11/site-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in ./env/lib/python3.11/site-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in ./env/lib/python3.11/site-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in ./env/lib/python3.11/site-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in ./env/lib/python3.11/site-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in ./env/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in ./env/lib/python3.11/site-packages (from seaborn) (2.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in ./env/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in ./env/lib/python3.11/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in ./env/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
            "Installing collected packages: seaborn\n",
            "Successfully installed seaborn-0.13.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers datasets evaluate accelerate peft trl bitsandbytes\n",
        "!pip install nvidia-ml-py3\n",
        "!pip install scikit-learn matplotlib seaborn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263",
      "metadata": {
        "id": "cca64f38-d8d2-4313-8295-fbbd43c2a263"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/joey/sp25-dl/project2/env/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2025-04-21 21:38:28.671428: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2025-04-21 21:38:28.717100: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745285908.739855  554292 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745285908.747516  554292 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1745285908.781585  554292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745285908.781617  554292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745285908.781619  554292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745285908.781620  554292 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-21 21:38:28.790852: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "from transformers import RobertaModel, RobertaTokenizer, TrainingArguments, Trainer, DataCollatorWithPadding, RobertaForSequenceClassification\n",
        "from peft import LoraConfig, get_peft_model, PeftModel\n",
        "from datasets import load_dataset, Dataset, ClassLabel\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59d6e377",
      "metadata": {
        "id": "59d6e377"
      },
      "source": [
        "## Load Tokenizer and Preprocess Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "21f42747-f551-40a5-a95f-7affb1eba4a3",
      "metadata": {
        "id": "21f42747-f551-40a5-a95f-7affb1eba4a3"
      },
      "outputs": [],
      "source": [
        "base_model = 'roberta-base'\n",
        "\n",
        "dataset = load_dataset('ag_news', split='train')\n",
        "tokenizer = RobertaTokenizer.from_pretrained(base_model)\n",
        "\n",
        "def preprocess(examples):\n",
        "    tokenized = tokenizer(examples['text'], truncation=True, padding=True)\n",
        "    return tokenized\n",
        "\n",
        "tokenized_dataset = dataset.map(preprocess, batched=True,  remove_columns=[\"text\"])\n",
        "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "9e07f641-bec0-43a6-8c26-510d7642916a",
      "metadata": {
        "id": "9e07f641-bec0-43a6-8c26-510d7642916a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of labels: 4\n",
            "the labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n"
          ]
        }
      ],
      "source": [
        "# Extract the number of classess and their names\n",
        "num_labels = dataset.features['label'].num_classes\n",
        "class_names = dataset.features[\"label\"].names\n",
        "print(f\"number of labels: {num_labels}\")\n",
        "print(f\"the labels: {class_names}\")\n",
        "\n",
        "# Create an id2label mapping\n",
        "# We will need this for our classifier.\n",
        "id2label = {i: label for i, label in enumerate(class_names)}\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0408894d",
      "metadata": {},
      "source": [
        "## Make train and eval split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "e7413430-be57-482b-856e-36bd4ba799df",
      "metadata": {
        "id": "e7413430-be57-482b-856e-36bd4ba799df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of train samples: 118720\n",
            "Number of eval samples: 1280\n"
          ]
        }
      ],
      "source": [
        "# Split the original training set\n",
        "split_datasets = tokenized_dataset.train_test_split(test_size=1280, seed=42)\n",
        "train_dataset = split_datasets['train']\n",
        "eval_dataset = split_datasets['test']\n",
        "\n",
        "print(\"Number of train samples:\", len(train_dataset))\n",
        "print(\"Number of eval samples:\", len(eval_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12284b58",
      "metadata": {
        "id": "12284b58"
      },
      "source": [
        "## Training Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1",
      "metadata": {
        "id": "0ee64c43-fe38-479a-b3c5-7d939a3db4c1"
      },
      "outputs": [],
      "source": [
        "# To track evaluation accuracy during training\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def compute_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(labels, preds)\n",
        "    return {\n",
        "        'accuracy': accuracy\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5",
      "metadata": {
        "id": "ebbc20a2-a1c0-4cb7-b842-f52e4de61ed5"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "\n",
        "def evaluate_model(inference_model, dataset, labelled=True, batch_size=8, data_collator=None):\n",
        "    \"\"\"\n",
        "    Evaluate a PEFT model on a dataset.\n",
        "\n",
        "    Args:\n",
        "        inference_model: The model to evaluate.\n",
        "        dataset: The dataset (Hugging Face Dataset) to run inference on.\n",
        "        labelled (bool): If True, the dataset includes labels and metrics will be computed.\n",
        "                         If False, only predictions will be returned.\n",
        "        batch_size (int): Batch size for inference.\n",
        "        data_collator: Function to collate batches. If None, the default collate_fn is used.\n",
        "\n",
        "    Returns:\n",
        "        If labelled is True, returns a tuple (metrics, predictions)\n",
        "        If labelled is False, returns the predictions.\n",
        "    \"\"\"\n",
        "    # Create the DataLoader\n",
        "    eval_dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=data_collator)\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    inference_model.to(device)\n",
        "    inference_model.eval()\n",
        "\n",
        "    all_predictions = []\n",
        "    if labelled:\n",
        "        metric = evaluate.load('accuracy')\n",
        "\n",
        "    # Loop over the DataLoader\n",
        "    for batch in tqdm(eval_dataloader):\n",
        "        # Move each tensor in the batch to the device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = inference_model(**batch)\n",
        "        predictions = outputs.logits.argmax(dim=-1)\n",
        "        all_predictions.append(predictions.cpu())\n",
        "\n",
        "        if labelled:\n",
        "            # Expecting that labels are provided under the \"labels\" key.\n",
        "            references = batch[\"labels\"]\n",
        "            metric.add_batch(\n",
        "                predictions=predictions.cpu().numpy(),\n",
        "                references=references.cpu().numpy()\n",
        "            )\n",
        "\n",
        "    # Concatenate predictions from all batches\n",
        "    all_predictions = torch.cat(all_predictions, dim=0)\n",
        "\n",
        "    if labelled:\n",
        "        eval_metric = metric.compute()\n",
        "        print(\"Evaluation Metric:\", eval_metric)\n",
        "        return eval_metric, all_predictions\n",
        "    else:\n",
        "        return all_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "652452e3",
      "metadata": {
        "id": "652452e3"
      },
      "source": [
        "## Design Space Exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9342bc22",
      "metadata": {},
      "source": [
        "### Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "7750e71f",
      "metadata": {},
      "outputs": [],
      "source": [
        "output_base_dir = \"dse_results\" # base directory for all DSE runs\n",
        "os.makedirs(output_base_dir, exist_ok=True)\n",
        "\n",
        "# hyperparameter ranges for DSE\n",
        "lora_ranks = [4, 5, 6, 7] \n",
        "lora_alpha_scaling = [1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7113c425",
      "metadata": {},
      "source": [
        "\n",
        "### Design Space Exploration Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876",
      "metadata": {
        "id": "bd0ca0ea-86b8-47f7-8cbf-83da25685876"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Starting Run: rank_4_alpha_4 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=4, alpha=4\n",
            "PEFT Model Configured:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:49, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.389200</td>\n",
              "      <td>1.383539</td>\n",
              "      <td>0.388281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.378700</td>\n",
              "      <td>1.374011</td>\n",
              "      <td>0.319531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.370700</td>\n",
              "      <td>1.364983</td>\n",
              "      <td>0.554688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.362100</td>\n",
              "      <td>1.354770</td>\n",
              "      <td>0.726562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.347600</td>\n",
              "      <td>1.341964</td>\n",
              "      <td>0.726562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.328900</td>\n",
              "      <td>1.323406</td>\n",
              "      <td>0.842969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.311400</td>\n",
              "      <td>1.302116</td>\n",
              "      <td>0.806250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.294100</td>\n",
              "      <td>1.274563</td>\n",
              "      <td>0.857031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.259200</td>\n",
              "      <td>1.242157</td>\n",
              "      <td>0.859375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.219300</td>\n",
              "      <td>1.200480</td>\n",
              "      <td>0.869531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.174200</td>\n",
              "      <td>1.154303</td>\n",
              "      <td>0.872656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>1.126200</td>\n",
              "      <td>1.109701</td>\n",
              "      <td>0.881250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>1.086300</td>\n",
              "      <td>1.071033</td>\n",
              "      <td>0.882031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>1.043400</td>\n",
              "      <td>1.039246</td>\n",
              "      <td>0.880469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.025000</td>\n",
              "      <td>1.020203</td>\n",
              "      <td>0.880469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>1.016600</td>\n",
              "      <td>1.013991</td>\n",
              "      <td>0.880469</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88046875}\n",
            "Run rank_4_alpha_4 completed. Accuracy: 0.8805\n",
            "\n",
            "--- Starting Run: rank_4_alpha_8 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=4, alpha=8\n",
            "PEFT Model Configured:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 25:03, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382500</td>\n",
              "      <td>1.379976</td>\n",
              "      <td>0.270313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.378200</td>\n",
              "      <td>1.370393</td>\n",
              "      <td>0.452344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.370700</td>\n",
              "      <td>1.359665</td>\n",
              "      <td>0.586719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.356700</td>\n",
              "      <td>1.343353</td>\n",
              "      <td>0.731250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.331000</td>\n",
              "      <td>1.315576</td>\n",
              "      <td>0.798438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.289700</td>\n",
              "      <td>1.263500</td>\n",
              "      <td>0.865625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.217400</td>\n",
              "      <td>1.171665</td>\n",
              "      <td>0.857031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.103300</td>\n",
              "      <td>1.013957</td>\n",
              "      <td>0.875781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.923600</td>\n",
              "      <td>0.790333</td>\n",
              "      <td>0.872656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.720700</td>\n",
              "      <td>0.611649</td>\n",
              "      <td>0.884375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.598900</td>\n",
              "      <td>0.519284</td>\n",
              "      <td>0.878125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.534000</td>\n",
              "      <td>0.467105</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.498100</td>\n",
              "      <td>0.440191</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.451100</td>\n",
              "      <td>0.423822</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.465100</td>\n",
              "      <td>0.416793</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.453700</td>\n",
              "      <td>0.414398</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88671875}\n",
            "Run rank_4_alpha_8 completed. Accuracy: 0.8867\n",
            "\n",
            "--- Starting Run: rank_4_alpha_12 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=4, alpha=12\n",
            "PEFT Model Configured:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379825</td>\n",
              "      <td>0.271875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377700</td>\n",
              "      <td>1.369415</td>\n",
              "      <td>0.461719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.368300</td>\n",
              "      <td>1.355699</td>\n",
              "      <td>0.629687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.348500</td>\n",
              "      <td>1.329476</td>\n",
              "      <td>0.781250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.303400</td>\n",
              "      <td>1.270891</td>\n",
              "      <td>0.832812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.202800</td>\n",
              "      <td>1.119587</td>\n",
              "      <td>0.871094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.981200</td>\n",
              "      <td>0.809283</td>\n",
              "      <td>0.863281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.726200</td>\n",
              "      <td>0.568083</td>\n",
              "      <td>0.882031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.566300</td>\n",
              "      <td>0.462727</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.467900</td>\n",
              "      <td>0.415150</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.435300</td>\n",
              "      <td>0.395688</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>0.379041</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.408500</td>\n",
              "      <td>0.372343</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.370400</td>\n",
              "      <td>0.367418</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.396900</td>\n",
              "      <td>0.365661</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.383500</td>\n",
              "      <td>0.364847</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88828125}\n",
            "Run rank_4_alpha_12 completed. Accuracy: 0.8883\n",
            "\n",
            "--- Starting Run: rank_4_alpha_16 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=4, alpha=16\n",
            "PEFT Model Configured:\n",
            "trainable params: 814,852 || all params: 125,463,560 || trainable%: 0.6495\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 25:07, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379663</td>\n",
              "      <td>0.273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377300</td>\n",
              "      <td>1.368571</td>\n",
              "      <td>0.474219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.366300</td>\n",
              "      <td>1.351611</td>\n",
              "      <td>0.659375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.339400</td>\n",
              "      <td>1.311400</td>\n",
              "      <td>0.814063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.261400</td>\n",
              "      <td>1.190594</td>\n",
              "      <td>0.857031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.039200</td>\n",
              "      <td>0.831494</td>\n",
              "      <td>0.868750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.704500</td>\n",
              "      <td>0.548062</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.541500</td>\n",
              "      <td>0.440186</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.462400</td>\n",
              "      <td>0.397454</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.402700</td>\n",
              "      <td>0.377380</td>\n",
              "      <td>0.893750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.390400</td>\n",
              "      <td>0.370098</td>\n",
              "      <td>0.884375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.385900</td>\n",
              "      <td>0.359197</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.382400</td>\n",
              "      <td>0.356273</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.344400</td>\n",
              "      <td>0.353362</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.374400</td>\n",
              "      <td>0.352594</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.361100</td>\n",
              "      <td>0.352109</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8890625}\n",
            "Run rank_4_alpha_16 completed. Accuracy: 0.8891\n",
            "\n",
            "--- Starting Run: rank_5_alpha_5 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=5, alpha=5\n",
            "PEFT Model Configured:\n",
            "trainable params: 870,148 || all params: 125,518,856 || trainable%: 0.6932\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:57, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382500</td>\n",
              "      <td>1.380091</td>\n",
              "      <td>0.269531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.378600</td>\n",
              "      <td>1.371076</td>\n",
              "      <td>0.445312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.372100</td>\n",
              "      <td>1.362262</td>\n",
              "      <td>0.552344</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.361400</td>\n",
              "      <td>1.350800</td>\n",
              "      <td>0.669531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.344100</td>\n",
              "      <td>1.335078</td>\n",
              "      <td>0.771875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.322400</td>\n",
              "      <td>1.311387</td>\n",
              "      <td>0.855469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.294200</td>\n",
              "      <td>1.280253</td>\n",
              "      <td>0.837500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.262000</td>\n",
              "      <td>1.237182</td>\n",
              "      <td>0.867188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.210200</td>\n",
              "      <td>1.181972</td>\n",
              "      <td>0.868750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.140600</td>\n",
              "      <td>1.107560</td>\n",
              "      <td>0.872656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>1.062900</td>\n",
              "      <td>1.022102</td>\n",
              "      <td>0.875781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.978500</td>\n",
              "      <td>0.939149</td>\n",
              "      <td>0.879687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.913000</td>\n",
              "      <td>0.868504</td>\n",
              "      <td>0.878906</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.841800</td>\n",
              "      <td>0.812868</td>\n",
              "      <td>0.879687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.815100</td>\n",
              "      <td>0.781146</td>\n",
              "      <td>0.879687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.796800</td>\n",
              "      <td>0.771036</td>\n",
              "      <td>0.879687</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8796875}\n",
            "Run rank_5_alpha_5 completed. Accuracy: 0.8797\n",
            "\n",
            "--- Starting Run: rank_5_alpha_10 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=5, alpha=10\n",
            "PEFT Model Configured:\n",
            "trainable params: 870,148 || all params: 125,518,856 || trainable%: 0.6932\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 25:21, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379902</td>\n",
              "      <td>0.271875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377900</td>\n",
              "      <td>1.369853</td>\n",
              "      <td>0.459375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.369400</td>\n",
              "      <td>1.357451</td>\n",
              "      <td>0.606250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.352500</td>\n",
              "      <td>1.336009</td>\n",
              "      <td>0.762500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.317700</td>\n",
              "      <td>1.293865</td>\n",
              "      <td>0.829688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.250600</td>\n",
              "      <td>1.200329</td>\n",
              "      <td>0.868750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.109300</td>\n",
              "      <td>1.002278</td>\n",
              "      <td>0.864844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.883700</td>\n",
              "      <td>0.709880</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.668800</td>\n",
              "      <td>0.537909</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.530800</td>\n",
              "      <td>0.456280</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.475600</td>\n",
              "      <td>0.420886</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.445800</td>\n",
              "      <td>0.396659</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.430600</td>\n",
              "      <td>0.385541</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.390000</td>\n",
              "      <td>0.377956</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.413000</td>\n",
              "      <td>0.374979</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.400400</td>\n",
              "      <td>0.373786</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8875}\n",
            "Run rank_5_alpha_10 completed. Accuracy: 0.8875\n",
            "\n",
            "--- Starting Run: rank_5_alpha_15 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=5, alpha=15\n",
            "PEFT Model Configured:\n",
            "trainable params: 870,148 || all params: 125,518,856 || trainable%: 0.6932\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:58, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379716</td>\n",
              "      <td>0.273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377400</td>\n",
              "      <td>1.368656</td>\n",
              "      <td>0.470313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.366600</td>\n",
              "      <td>1.352188</td>\n",
              "      <td>0.650781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.341000</td>\n",
              "      <td>1.314840</td>\n",
              "      <td>0.808594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.270600</td>\n",
              "      <td>1.207563</td>\n",
              "      <td>0.858594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.073000</td>\n",
              "      <td>0.879516</td>\n",
              "      <td>0.870313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.745900</td>\n",
              "      <td>0.577302</td>\n",
              "      <td>0.881250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.565700</td>\n",
              "      <td>0.452614</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.475700</td>\n",
              "      <td>0.402215</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.410100</td>\n",
              "      <td>0.378950</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.397000</td>\n",
              "      <td>0.370207</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.389300</td>\n",
              "      <td>0.358862</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.385100</td>\n",
              "      <td>0.355479</td>\n",
              "      <td>0.889844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.346900</td>\n",
              "      <td>0.352278</td>\n",
              "      <td>0.889844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.375800</td>\n",
              "      <td>0.351341</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.362300</td>\n",
              "      <td>0.350790</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.89140625}\n",
            "Run rank_5_alpha_15 completed. Accuracy: 0.8914\n",
            "\n",
            "--- Starting Run: rank_5_alpha_20 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=5, alpha=20\n",
            "PEFT Model Configured:\n",
            "trainable params: 870,148 || all params: 125,518,856 || trainable%: 0.6932\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382300</td>\n",
              "      <td>1.379519</td>\n",
              "      <td>0.271094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.376600</td>\n",
              "      <td>1.367073</td>\n",
              "      <td>0.486719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.362600</td>\n",
              "      <td>1.344364</td>\n",
              "      <td>0.704688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.322000</td>\n",
              "      <td>1.274340</td>\n",
              "      <td>0.842187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.164300</td>\n",
              "      <td>0.982007</td>\n",
              "      <td>0.861719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.789500</td>\n",
              "      <td>0.582713</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.542000</td>\n",
              "      <td>0.445232</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.465700</td>\n",
              "      <td>0.394729</td>\n",
              "      <td>0.892969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.420500</td>\n",
              "      <td>0.372332</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.373800</td>\n",
              "      <td>0.360742</td>\n",
              "      <td>0.896875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.370600</td>\n",
              "      <td>0.357417</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.369900</td>\n",
              "      <td>0.348480</td>\n",
              "      <td>0.892969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.369900</td>\n",
              "      <td>0.346952</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.330900</td>\n",
              "      <td>0.344714</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.362300</td>\n",
              "      <td>0.344186</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.350100</td>\n",
              "      <td>0.343762</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.29s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.89140625}\n",
            "Run rank_5_alpha_20 completed. Accuracy: 0.8914\n",
            "\n",
            "--- Starting Run: rank_6_alpha_6 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=6, alpha=6\n",
            "PEFT Model Configured:\n",
            "trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:44, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382500</td>\n",
              "      <td>1.380077</td>\n",
              "      <td>0.265625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.378600</td>\n",
              "      <td>1.371000</td>\n",
              "      <td>0.450781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.372000</td>\n",
              "      <td>1.361856</td>\n",
              "      <td>0.553125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.360800</td>\n",
              "      <td>1.349565</td>\n",
              "      <td>0.678125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.341900</td>\n",
              "      <td>1.331857</td>\n",
              "      <td>0.776563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.317000</td>\n",
              "      <td>1.303474</td>\n",
              "      <td>0.860938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.282000</td>\n",
              "      <td>1.263135</td>\n",
              "      <td>0.845313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.237200</td>\n",
              "      <td>1.203205</td>\n",
              "      <td>0.872656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.164100</td>\n",
              "      <td>1.120505</td>\n",
              "      <td>0.870313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.059800</td>\n",
              "      <td>1.005638</td>\n",
              "      <td>0.875781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.942500</td>\n",
              "      <td>0.874099</td>\n",
              "      <td>0.874219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.826200</td>\n",
              "      <td>0.757342</td>\n",
              "      <td>0.881250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.745200</td>\n",
              "      <td>0.674046</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.668800</td>\n",
              "      <td>0.620269</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.651000</td>\n",
              "      <td>0.594257</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.633800</td>\n",
              "      <td>0.586264</td>\n",
              "      <td>0.884375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.27s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.884375}\n",
            "Run rank_6_alpha_6 completed. Accuracy: 0.8844\n",
            "\n",
            "--- Starting Run: rank_6_alpha_12 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=6, alpha=12\n",
            "PEFT Model Configured:\n",
            "trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379839</td>\n",
              "      <td>0.272656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377800</td>\n",
              "      <td>1.369601</td>\n",
              "      <td>0.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.368700</td>\n",
              "      <td>1.356128</td>\n",
              "      <td>0.621875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.349500</td>\n",
              "      <td>1.330694</td>\n",
              "      <td>0.778125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.306200</td>\n",
              "      <td>1.274591</td>\n",
              "      <td>0.842187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.209400</td>\n",
              "      <td>1.127160</td>\n",
              "      <td>0.871094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.990100</td>\n",
              "      <td>0.815176</td>\n",
              "      <td>0.867969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.730700</td>\n",
              "      <td>0.569564</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.567900</td>\n",
              "      <td>0.461012</td>\n",
              "      <td>0.882031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.465400</td>\n",
              "      <td>0.411694</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.432200</td>\n",
              "      <td>0.392559</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.415500</td>\n",
              "      <td>0.375625</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.405300</td>\n",
              "      <td>0.369285</td>\n",
              "      <td>0.884375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.367800</td>\n",
              "      <td>0.364441</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.393100</td>\n",
              "      <td>0.362649</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.378700</td>\n",
              "      <td>0.361863</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88671875}\n",
            "Run rank_6_alpha_12 completed. Accuracy: 0.8867\n",
            "\n",
            "--- Starting Run: rank_6_alpha_18 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=6, alpha=18\n",
            "PEFT Model Configured:\n",
            "trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:38, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382300</td>\n",
              "      <td>1.379616</td>\n",
              "      <td>0.272656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377100</td>\n",
              "      <td>1.367993</td>\n",
              "      <td>0.475781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.364900</td>\n",
              "      <td>1.348720</td>\n",
              "      <td>0.681250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.332900</td>\n",
              "      <td>1.297932</td>\n",
              "      <td>0.828125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.225400</td>\n",
              "      <td>1.110211</td>\n",
              "      <td>0.861719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.913000</td>\n",
              "      <td>0.681486</td>\n",
              "      <td>0.879687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.608100</td>\n",
              "      <td>0.482098</td>\n",
              "      <td>0.881250</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.493700</td>\n",
              "      <td>0.410057</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.435000</td>\n",
              "      <td>0.380973</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.382100</td>\n",
              "      <td>0.366213</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.375600</td>\n",
              "      <td>0.361860</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.374800</td>\n",
              "      <td>0.352270</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.372700</td>\n",
              "      <td>0.350741</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.335100</td>\n",
              "      <td>0.348263</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.364500</td>\n",
              "      <td>0.347622</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.350600</td>\n",
              "      <td>0.347202</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8875}\n",
            "Run rank_6_alpha_18 completed. Accuracy: 0.8875\n",
            "\n",
            "--- Starting Run: rank_6_alpha_24 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=6, alpha=24\n",
            "PEFT Model Configured:\n",
            "trainable params: 925,444 || all params: 125,574,152 || trainable%: 0.7370\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382300</td>\n",
              "      <td>1.379535</td>\n",
              "      <td>0.273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.376500</td>\n",
              "      <td>1.366943</td>\n",
              "      <td>0.474219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.361600</td>\n",
              "      <td>1.341418</td>\n",
              "      <td>0.717187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.311400</td>\n",
              "      <td>1.247735</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.087400</td>\n",
              "      <td>0.837407</td>\n",
              "      <td>0.864062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.679900</td>\n",
              "      <td>0.506935</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.489900</td>\n",
              "      <td>0.415279</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.436700</td>\n",
              "      <td>0.378751</td>\n",
              "      <td>0.889844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.402300</td>\n",
              "      <td>0.362760</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.357800</td>\n",
              "      <td>0.353359</td>\n",
              "      <td>0.892969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.358100</td>\n",
              "      <td>0.351963</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.360400</td>\n",
              "      <td>0.343420</td>\n",
              "      <td>0.895312</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.360500</td>\n",
              "      <td>0.343118</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.322400</td>\n",
              "      <td>0.340818</td>\n",
              "      <td>0.888281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.351200</td>\n",
              "      <td>0.340537</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.337700</td>\n",
              "      <td>0.340234</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.89140625}\n",
            "Run rank_6_alpha_24 completed. Accuracy: 0.8914\n",
            "\n",
            "--- Starting Run: rank_7_alpha_7 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=7, alpha=7\n",
            "PEFT Model Configured:\n",
            "trainable params: 980,740 || all params: 125,629,448 || trainable%: 0.7807\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:38, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382500</td>\n",
              "      <td>1.380028</td>\n",
              "      <td>0.268750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.378400</td>\n",
              "      <td>1.370747</td>\n",
              "      <td>0.450000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.371400</td>\n",
              "      <td>1.360977</td>\n",
              "      <td>0.565625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.359200</td>\n",
              "      <td>1.346997</td>\n",
              "      <td>0.700000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.337400</td>\n",
              "      <td>1.325370</td>\n",
              "      <td>0.789062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.306000</td>\n",
              "      <td>1.287992</td>\n",
              "      <td>0.863281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>1.257500</td>\n",
              "      <td>1.229877</td>\n",
              "      <td>0.854688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>1.188600</td>\n",
              "      <td>1.137259</td>\n",
              "      <td>0.871875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>1.075000</td>\n",
              "      <td>1.001371</td>\n",
              "      <td>0.870313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.914400</td>\n",
              "      <td>0.818176</td>\n",
              "      <td>0.880469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.756100</td>\n",
              "      <td>0.664517</td>\n",
              "      <td>0.875781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.650500</td>\n",
              "      <td>0.573414</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.591300</td>\n",
              "      <td>0.521861</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.532100</td>\n",
              "      <td>0.491076</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.533500</td>\n",
              "      <td>0.477225</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.521200</td>\n",
              "      <td>0.472826</td>\n",
              "      <td>0.889062</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8890625}\n",
            "Run rank_7_alpha_7 completed. Accuracy: 0.8891\n",
            "\n",
            "--- Starting Run: rank_7_alpha_14 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=7, alpha=14\n",
            "PEFT Model Configured:\n",
            "trainable params: 980,740 || all params: 125,629,448 || trainable%: 0.7807\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:37, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379775</td>\n",
              "      <td>0.271875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.377500</td>\n",
              "      <td>1.369081</td>\n",
              "      <td>0.467969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.367500</td>\n",
              "      <td>1.354156</td>\n",
              "      <td>0.639062</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.345100</td>\n",
              "      <td>1.322984</td>\n",
              "      <td>0.799219</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.288600</td>\n",
              "      <td>1.243766</td>\n",
              "      <td>0.848437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>1.143700</td>\n",
              "      <td>1.008391</td>\n",
              "      <td>0.869531</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.844700</td>\n",
              "      <td>0.656723</td>\n",
              "      <td>0.871094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.617100</td>\n",
              "      <td>0.487302</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.501700</td>\n",
              "      <td>0.419914</td>\n",
              "      <td>0.884375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.424400</td>\n",
              "      <td>0.389509</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.405900</td>\n",
              "      <td>0.378349</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.396700</td>\n",
              "      <td>0.365143</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.390400</td>\n",
              "      <td>0.361064</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.353200</td>\n",
              "      <td>0.357596</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.378500</td>\n",
              "      <td>0.356396</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.364400</td>\n",
              "      <td>0.355769</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88671875}\n",
            "Run rank_7_alpha_14 completed. Accuracy: 0.8867\n",
            "\n",
            "--- Starting Run: rank_7_alpha_21 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=7, alpha=21\n",
            "PEFT Model Configured:\n",
            "trainable params: 980,740 || all params: 125,629,448 || trainable%: 0.7807\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:39, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382400</td>\n",
              "      <td>1.379520</td>\n",
              "      <td>0.270313</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.376700</td>\n",
              "      <td>1.367364</td>\n",
              "      <td>0.489063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.363000</td>\n",
              "      <td>1.345208</td>\n",
              "      <td>0.704688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.322700</td>\n",
              "      <td>1.276606</td>\n",
              "      <td>0.843750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.162400</td>\n",
              "      <td>0.975317</td>\n",
              "      <td>0.860156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.773200</td>\n",
              "      <td>0.566634</td>\n",
              "      <td>0.883594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.527400</td>\n",
              "      <td>0.438160</td>\n",
              "      <td>0.882812</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.455500</td>\n",
              "      <td>0.391376</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.413200</td>\n",
              "      <td>0.371118</td>\n",
              "      <td>0.885156</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.366100</td>\n",
              "      <td>0.360353</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.364400</td>\n",
              "      <td>0.357584</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.366200</td>\n",
              "      <td>0.348645</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.365500</td>\n",
              "      <td>0.347531</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.327700</td>\n",
              "      <td>0.345433</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.355300</td>\n",
              "      <td>0.344976</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.343100</td>\n",
              "      <td>0.344572</td>\n",
              "      <td>0.886719</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.28s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.88671875}\n",
            "Run rank_7_alpha_21 completed. Accuracy: 0.8867\n",
            "\n",
            "--- Starting Run: rank_7_alpha_28 ---\n",
            "Loading base model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuring LoRA with r=7, alpha=28\n",
            "PEFT Model Configured:\n",
            "trainable params: 980,740 || all params: 125,629,448 || trainable%: 0.7807\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1600' max='1600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1600/1600 24:48, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.382200</td>\n",
              "      <td>1.379219</td>\n",
              "      <td>0.273438</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.375800</td>\n",
              "      <td>1.365332</td>\n",
              "      <td>0.498437</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>1.357000</td>\n",
              "      <td>1.332309</td>\n",
              "      <td>0.764844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>1.279500</td>\n",
              "      <td>1.169600</td>\n",
              "      <td>0.854688</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.941600</td>\n",
              "      <td>0.671060</td>\n",
              "      <td>0.868750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.574600</td>\n",
              "      <td>0.452959</td>\n",
              "      <td>0.885938</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.445500</td>\n",
              "      <td>0.396441</td>\n",
              "      <td>0.887500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.415800</td>\n",
              "      <td>0.370583</td>\n",
              "      <td>0.889844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.388600</td>\n",
              "      <td>0.358791</td>\n",
              "      <td>0.889844</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.347600</td>\n",
              "      <td>0.351612</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.350500</td>\n",
              "      <td>0.350974</td>\n",
              "      <td>0.891406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.354800</td>\n",
              "      <td>0.342556</td>\n",
              "      <td>0.892969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.355800</td>\n",
              "      <td>0.342315</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.318300</td>\n",
              "      <td>0.340265</td>\n",
              "      <td>0.890625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.339975</td>\n",
              "      <td>0.892969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1600</td>\n",
              "      <td>0.333800</td>\n",
              "      <td>0.339622</td>\n",
              "      <td>0.892188</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training finished.\n",
            "Evaluating model on evaluation set...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:25<00:00,  1.26s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metric: {'accuracy': 0.8921875}\n",
            "Run rank_7_alpha_28 completed. Accuracy: 0.8922\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import gc # Garbage collector for potentially clearing GPU memory\n",
        "\n",
        "results = []\n",
        "\n",
        "for rank in lora_ranks:\n",
        "    for alpha_scale in lora_alpha_scaling:\n",
        "        alpha = rank * alpha_scale\n",
        "        run_name = f\"rank_{rank}_alpha_{alpha}\"\n",
        "        print(f\"\\n--- Starting Run: {run_name} ---\")\n",
        "\n",
        "        # Define output directory for this specific run\n",
        "        current_output_dir = os.path.join(output_base_dir, run_name)\n",
        "        os.makedirs(current_output_dir, exist_ok=True)\n",
        "\n",
        "        # 1. Load Base Model (Load fresh for each run)\n",
        "        print(\"Loading base model...\")\n",
        "\n",
        "        model = RobertaForSequenceClassification.from_pretrained(\n",
        "            base_model,\n",
        "            id2label=id2label)\n",
        "\n",
        "        # Move model to GPU if possible\n",
        "        if torch.cuda.is_available():\n",
        "            model.to('cuda')\n",
        "\n",
        "        # Configure LoRA\n",
        "        print(f\"Configuring LoRA with r={rank}, alpha={alpha}\")\n",
        "        peft_config = LoraConfig(\n",
        "            r=rank,  # LoRA rank\n",
        "            lora_alpha=alpha,  # Alpha parameter for scaling\n",
        "            lora_dropout=0.1, # Dropout probability for LoRA layers\n",
        "            target_modules=[\"query\", \"key\", \"value\"], # Apply LoRA to these layers\n",
        "            bias=\"none\",  # Don't train bias parameters\n",
        "            task_type=\"SEQ_CLS\", # Specify the task type\n",
        "        )\n",
        "\n",
        "        peft_model = get_peft_model(model, peft_config)\n",
        "\n",
        "        print(\"PEFT Model Configured:\")\n",
        "        peft_model.print_trainable_parameters()\n",
        "\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=current_output_dir,\n",
        "            report_to=None,\n",
        "            eval_strategy=\"steps\",\n",
        "            logging_steps=100,\n",
        "            learning_rate=1e-5,\n",
        "            max_steps=1600,\n",
        "            num_train_epochs=1,\n",
        "            use_cpu=False,\n",
        "            dataloader_num_workers=4,\n",
        "            per_device_train_batch_size=16,\n",
        "            per_device_eval_batch_size=64, # or 128\n",
        "            optim=\"adamw_torch\",\n",
        "            gradient_checkpointing=False,\n",
        "            gradient_checkpointing_kwargs={'use_reentrant': True},\n",
        "            load_best_model_at_end=True,\n",
        "            metric_for_best_model=\"eval_loss\",\n",
        "            greater_is_better=False\n",
        "        )\n",
        "\n",
        "        trainer = Trainer(\n",
        "            model=peft_model,\n",
        "            args=training_args,\n",
        "            compute_metrics=compute_metrics,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "        )\n",
        "\n",
        "        # 6. Train the model\n",
        "        print(\"Starting training...\")\n",
        "        try:\n",
        "            train_result = trainer.train()\n",
        "            print(\"Training finished.\")\n",
        "            trainer.save_model()\n",
        "\n",
        "            # 7. Evaluate the model after training\n",
        "            print(\"Evaluating model on evaluation set...\")\n",
        "            eval_metrics, _ = evaluate_model(\n",
        "                peft_model,\n",
        "                eval_dataset,\n",
        "                labelled=True,\n",
        "                batch_size=training_args.per_device_eval_batch_size,\n",
        "                data_collator=data_collator\n",
        "            )\n",
        "            final_accuracy = eval_metrics.get('accuracy', float('nan'))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! ERROR during training/evaluation for {run_name}: {e}\")\n",
        "            final_accuracy = float('nan')  # Record failure\n",
        "\n",
        "        # 8. Store results\n",
        "        results.append({\n",
        "            \"lora_rank\": rank,\n",
        "            \"lora_alpha\": alpha,\n",
        "            \"accuracy\": final_accuracy,\n",
        "            \"output_dir\": current_output_dir\n",
        "        })\n",
        "        print(f\"Run {run_name} completed. Accuracy: {final_accuracy:.4f}\")\n",
        "\n",
        "        # 9. Clean up memory (Important!)\n",
        "        del model\n",
        "        del peft_model\n",
        "        del trainer\n",
        "        gc.collect()\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6ffef8",
      "metadata": {},
      "source": [
        "## Post-DSE Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19906cb4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "    lora_rank  lora_alpha  accuracy                   output_dir\n",
            "15          7          28  0.892188  dse_results/rank_7_alpha_28\n",
            "6           5          15  0.891406  dse_results/rank_5_alpha_15\n",
            "11          6          24  0.891406  dse_results/rank_6_alpha_24\n",
            "7           5          20  0.891406  dse_results/rank_5_alpha_20\n",
            "3           4          16  0.889062  dse_results/rank_4_alpha_16\n",
            "12          7           7  0.889062   dse_results/rank_7_alpha_7\n",
            "2           4          12  0.888281  dse_results/rank_4_alpha_12\n",
            "5           5          10  0.887500  dse_results/rank_5_alpha_10\n",
            "10          6          18  0.887500  dse_results/rank_6_alpha_18\n",
            "13          7          14  0.886719  dse_results/rank_7_alpha_14\n",
            "1           4           8  0.886719   dse_results/rank_4_alpha_8\n",
            "9           6          12  0.886719  dse_results/rank_6_alpha_12\n",
            "14          7          21  0.886719  dse_results/rank_7_alpha_21\n",
            "8           6           6  0.884375   dse_results/rank_6_alpha_6\n",
            "0           4           4  0.880469   dse_results/rank_4_alpha_4\n",
            "4           5           5  0.879687   dse_results/rank_5_alpha_5\n",
            "\n",
            "Full DSE results saved to: dse_results/dse_summary.csv\n"
          ]
        }
      ],
      "source": [
        "# Convert results to DataFrame for easy viewing/sorting\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"accuracy\", ascending=False)\n",
        "\n",
        "print(results_df)\n",
        "\n",
        "# Save results to CSV\n",
        "results_csv_path = os.path.join(output_base_dir, \"dse_summary.csv\")\n",
        "results_df.to_csv(results_csv_path, index=False)\n",
        "print(f\"\\nFull DSE results saved to: {results_csv_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "683e84e3",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x400 with 2 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "results_csv_path = os.path.join(\"dse_results_do0.1\", \"dse_summary.csv\")\n",
        "df = pd.read_csv(results_csv_path)\n",
        "\n",
        "# Add lora_alpha_multiplier column\n",
        "df[\"lora_alpha_multiplier\"] = df[\"lora_alpha\"] // df[\"lora_rank\"]\n",
        "\n",
        "# Pivot the data\n",
        "pivot = df.pivot(index=\"lora_rank\", columns=\"lora_alpha_multiplier\", values=\"accuracy\")\n",
        "\n",
        "# Plot heatmap\n",
        "plt.figure(figsize=(8, 4))\n",
        "sns.heatmap(pivot, annot=True, fmt=\".4f\", cmap=\"viridis\", cbar_kws={'label': 'Accuracy'})\n",
        "plt.title(\"Accuracy Heatmap by LoRA Rank and Alpha Multiplier\")\n",
        "plt.xlabel(r\"LoRA Alpha Multiplier ($\\alpha$/rank)\")\n",
        "plt.ylabel(\"LoRA Rank\")\n",
        "plt.tight_layout()\n",
        "plt.savefig('dse_heatmap.pdf')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203",
      "metadata": {
        "id": "75f39087-f2bb-49d3-9fe1-0d812fb30203"
      },
      "source": [
        "### Run Inference on unlabelled dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b33f4a5",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading best model from: dse_results/rank_4_alpha_8\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "RobertaForSequenceClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): RobertaClassificationHead(\n",
              "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load best model from DSE\n",
        "model_path = \"dse_results/rank_6_alpha_6\"\n",
        "\n",
        "print(f\"Loading best model from: {model_path}\")\n",
        "# Load the base model again\n",
        "base_inference_model = RobertaForSequenceClassification.from_pretrained(\n",
        "    base_model,\n",
        "    id2label=id2label,\n",
        "    num_labels=num_labels\n",
        ")\n",
        "# Load the PEFT adapter\n",
        "inference_model = PeftModel.from_pretrained(base_inference_model, model_path)\n",
        "inference_model.merge_and_unload() # Optional: Merge adapter weights for potentially faster inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7",
      "metadata": {
        "id": "2af62541-2c33-4f16-bb1c-cc969c715cd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 8000/8000 [00:02<00:00, 3499.23 examples/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 8000\n",
              "})"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Load your unlabelled data\n",
        "unlabelled_dataset = pd.read_pickle(\"test_unlabelled.pkl\")\n",
        "test_dataset = unlabelled_dataset.map(preprocess, batched=True, remove_columns=[\"text\"])\n",
        "unlabelled_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "e60991d3-38b1-4657-8854-408ce66f6b84",
      "metadata": {
        "id": "e60991d3-38b1-4657-8854-408ce66f6b84"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1000/1000 [01:38<00:00, 10.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inference complete. Predictions saved to inference_output.csv\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Run inference and save predictions\n",
        "preds = evaluate_model(inference_model, test_dataset, False, 8, data_collator)\n",
        "df_output = pd.DataFrame({\n",
        "    'ID': range(len(preds)),\n",
        "    'Label': preds.numpy()  # or preds.tolist()\n",
        "})\n",
        "df_output.to_csv(os.path.join(model_path,\"inference_output.csv\"), index=False)\n",
        "print(\"Inference complete. Predictions saved to inference_output.csv\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
